{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mobilenetv3 import MobileNetV3Extractor\n",
    "from models.lstm_attention import BiLSTMWithAttention\n",
    "from preprocessing.dataset import SignLanguageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    # resize to the standard input size\n",
    "    A.Resize(224, 224),  \n",
    "    # A.RandomCrop(200, 200),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    #A.Cutout(num_holes=2, max_h_size=40, max_w_size=40, fill_value=0),\n",
    "     # resize it back to 224 to prevent inconsistent dimensions after enhancement\n",
    "    A.Resize(112, 112), \n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = \"../data/processed_pt/train\"\n",
    "#class_names = sorted(os.listdir(train_root))\n",
    "class_names = sorted([\n",
    "    d for d in os.listdir(train_root)\n",
    "    if os.path.isdir(os.path.join(train_root, d)) and not d.startswith(\".\")\n",
    "]) \n",
    "\n",
    "label_map = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "train_dataset = SignLanguageDataset(\n",
    "    root_dir=train_root,\n",
    "    label_map=label_map,\n",
    "    transform=None \n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullSLRModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = MobileNetV3Extractor()\n",
    "        self.temporal_model = BiLSTMWithAttention(\n",
    "            input_dim=960, hidden_dim=256, num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C, H, W]\n",
    "        features = self.feature_extractor(x)          # [B, T, 960]\n",
    "        logits, _ = self.temporal_model(features)     # [B, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/slr-env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/slr-env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FullSLRModel(num_classes=len(label_map)).to(device)\n",
    "\n",
    "# Label smoothing is helpful for generalization and reduces overfitting\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "feature_extractor = MobileNetV3Extractor().to(device)\n",
    "sequence_model = BiLSTMWithAttention(input_dim=960, hidden_dim=256, num_classes=len(label_map)).to(device)\n",
    "\n",
    "model = nn.Sequential(feature_extractor, sequence_model).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Training:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] Training: 100%|██████████| 2/2 [00:31<00:00, 15.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1/3 | Loss: 5.6665 | Acc: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] Training: 100%|██████████| 2/2 [00:31<00:00, 15.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 2/3 | Loss: 5.4269 | Acc: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/3] Training: 100%|██████████| 2/2 [00:23<00:00, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 3/3 | Loss: 5.1600 | Acc: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for videos, labels in tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\"):\n",
    "        videos, labels = videos.to(device), labels.to(device)  # [B, T, C, H, W]\n",
    "\n",
    "        # \n",
    "        features = feature_extractor(videos)        # [B, T, D]\n",
    "        outputs, attn = sequence_model(features)    # [B, num_classes]\n",
    "\n",
    "        # \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # \n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f\"[Train] Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../checkpoints\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"../checkpoints/mobilenetv3_lstm_aug_smooth.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slr-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
